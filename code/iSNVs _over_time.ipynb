{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iSNV frequencies over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import seaborn as sns; sns.set()\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = 'CA04_ferret3'\n",
    "contact = 'CA04_ferret6'\n",
    "pair = ''\n",
    "\n",
    "# enter which timepoints are going to be plotted\n",
    "# if there is no data or if you wish to exclude data, enter 'None'\n",
    "\n",
    "timepoint_index1 = 'day1'\n",
    "timepoint_index2 = 'day3'\n",
    "timepoint_index3 = 'day5'\n",
    "timepoint_index4 = None\n",
    "\n",
    "timepoint_contact1 = 'day3'\n",
    "timepoint_contact2 = 'day5'\n",
    "timepoint_contact3 = None\n",
    "timepoint_contact4 = None\n",
    "\n",
    "#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclude timepoint_index4\n"
     ]
    }
   ],
   "source": [
    "# load SNPs from the index timepoints into a Pandas dataframe\n",
    "if timepoint_index1 == None:\n",
    "    print(\"exclude timepoint_index1\")\n",
    "    DF_index1 = pd.DataFrame()\n",
    "    DF_index1 = DF_index1.fillna(0)\n",
    "else: \n",
    "    DF_index1 = pd.read_csv(\n",
    "        filepath_or_buffer='../data_derived/vcfs-intersection/' + index + '_' + timepoint_index1 + '_intersection.csv',\n",
    "        header=0,\n",
    "        sep='\\t', \n",
    "        low_memory=False,\n",
    "        )\n",
    "\n",
    "# drop columns that are unneeded and reorder the columns\n",
    "    DF_index1 = DF_index1[['SNP', '%']]\n",
    "    DF_index1 = DF_index1.rename(columns={'%':timepoint_index1})\n",
    "\n",
    "####################\n",
    "\n",
    "if timepoint_index2 == None:\n",
    "    print(\"exclude timepoint_index2\")\n",
    "    DF_index2 = pd.DataFrame()\n",
    "    DF_index2 = DF_index2.fillna(0)\n",
    "else: \n",
    "    DF_index2 = pd.read_csv(\n",
    "        filepath_or_buffer='../data_derived/vcfs-intersection/' + index + '_' + timepoint_index2 + '_intersection.csv',\n",
    "        header=0,\n",
    "        sep='\\t', \n",
    "        low_memory=False,\n",
    "        )\n",
    "\n",
    "# drop columns that are unneeded and reorder the columns\n",
    "    DF_index2 = DF_index2[['SNP', '%']]\n",
    "    DF_index2 = DF_index2.rename(columns={'%':timepoint_index2})\n",
    "    \n",
    "####################\n",
    "\n",
    "if timepoint_index3 == None:\n",
    "    print(\"exclude timepoint_index3\")\n",
    "    DF_index3 = pd.DataFrame()\n",
    "    DF_index3 = DF_index3.fillna(0)\n",
    "else: \n",
    "    DF_index3 = pd.read_csv(\n",
    "        filepath_or_buffer='../data_derived/vcfs-intersection/' + index + '_' + timepoint_index3 + '_intersection.csv',\n",
    "        header=0,\n",
    "        sep='\\t', \n",
    "        low_memory=False,\n",
    "        )\n",
    "\n",
    "# drop columns that are unneeded and reorder the columns\n",
    "    DF_index3 = DF_index3[['SNP', '%']]\n",
    "    DF_index3 = DF_index3.rename(columns={'%':timepoint_index3})\n",
    "    \n",
    "####################\n",
    "\n",
    "if timepoint_index4 == None:\n",
    "    print(\"exclude timepoint_index4\")\n",
    "    DF_index4 = pd.DataFrame()\n",
    "    DF_index4 = DF_index4.fillna(0)\n",
    "else: \n",
    "    DF_index4 = pd.read_csv(\n",
    "        filepath_or_buffer='../data_derived/vcfs-intersection/' + index + '_' + timepoint_index4 + '_intersection.csv',\n",
    "        header=0,\n",
    "        sep='\\t', \n",
    "        low_memory=False,\n",
    "        )\n",
    "\n",
    "# drop columns that are unneeded and reorder the columns\n",
    "    DF_index4 = DF_index4[['SNP', '%']]\n",
    "    DF_index4 = DF_index4.rename(columns={'%':timepoint_index4})\n",
    "\n",
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclude timepoint_contact3\n",
      "exclude timepoint_contact4\n"
     ]
    }
   ],
   "source": [
    "# load SNPs from the contact timepoints into a Pandas dataframe\n",
    "if timepoint_contact1 == None:\n",
    "    print(\"exclude timepoint_contact1\")\n",
    "    DF_contact1 = pd.DataFrame()\n",
    "    DF_contact1 = DF_contact1.fillna(0)\n",
    "else: \n",
    "    DF_contact1 = pd.read_csv(\n",
    "        filepath_or_buffer='../data_derived/vcfs-intersection/' + contact + '_' + timepoint_contact1 + '_intersection.csv',\n",
    "        header=0,\n",
    "        sep='\\t', \n",
    "        low_memory=False,\n",
    "        )\n",
    "\n",
    "# drop columns that are unneeded and reorder the columns\n",
    "    DF_contact1 = DF_contact1[['SNP', '%']]\n",
    "    DF_contact1 = DF_contact1.rename(columns={'%':timepoint_contact1})\n",
    "\n",
    "####################\n",
    "\n",
    "if timepoint_contact2 == None:\n",
    "    print(\"exclude timepoint_contact2\")\n",
    "    DF_contact2 = pd.DataFrame()\n",
    "    DF_contact2 = DF_contact2.fillna(0)\n",
    "else: \n",
    "    DF_contact2 = pd.read_csv(\n",
    "        filepath_or_buffer='../data_derived/vcfs-intersection/' + contact + '_' + timepoint_contact2 + '_intersection.csv',\n",
    "        header=0,\n",
    "        sep='\\t', \n",
    "        low_memory=False,\n",
    "        )\n",
    "\n",
    "# drop columns that are unneeded and reorder the columns\n",
    "    DF_contact2 = DF_contact2[['SNP', '%']]\n",
    "    DF_contact2 = DF_contact2.rename(columns={'%':timepoint_contact2})\n",
    "    \n",
    "####################\n",
    "\n",
    "if timepoint_contact3 == None:\n",
    "    print(\"exclude timepoint_contact3\")\n",
    "    DF_contact3 = pd.DataFrame()\n",
    "    DF_contact3 = DF_contact3.fillna(0)\n",
    "else: \n",
    "    DF_contact3 = pd.read_csv(\n",
    "        filepath_or_buffer='../data_derived/vcfs-intersection/' + contact + '_' + timepoint_contact3 + '_intersection.csv',\n",
    "        header=0,\n",
    "        sep='\\t', \n",
    "        low_memory=False,\n",
    "        )\n",
    "\n",
    "# drop columns that are unneeded and reorder the columns\n",
    "    DF_contact3 = DF_contact3[['SNP', '%']]\n",
    "    DF_contact3 = DF_contact3.rename(columns={'%':timepoint_contact3})\n",
    "    \n",
    "####################\n",
    "\n",
    "if timepoint_contact4 == None:\n",
    "    print(\"exclude timepoint_contact4\")\n",
    "    DF_contact4 = pd.DataFrame()\n",
    "    DF_contact4 = DF_contact4.fillna(0)\n",
    "else: \n",
    "    DF_contact4 = pd.read_csv(\n",
    "        filepath_or_buffer='../data_derived/vcfs-intersection/' + contact + '_' + timepoint_contact4 + '_intersection.csv',\n",
    "        header=0,\n",
    "        sep='\\t', \n",
    "        low_memory=False,\n",
    "        )\n",
    "\n",
    "# drop columns that are unneeded and reorder the columns\n",
    "    DF_contact4 = DF_contact4[['SNP', '%']]\n",
    "    DF_contact4 = DF_contact4.rename(columns={'%':timepoint_contact4})\n",
    "\n",
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequentially merge each timepoint\n",
    "# outer = merge from right and left \n",
    "\n",
    "if DF_index1.empty or DF_index2.empty: \n",
    "    pass\n",
    "else: \n",
    "    DF_index = pd.merge(DF_index1, DF_index2, how='outer', on=['SNP'])\n",
    "\n",
    "# merge DF with third index timepoint \n",
    "if DF_index.empty or DF_index3.empty: \n",
    "    pass\n",
    "else: \n",
    "    DF_index = pd.merge(DF_index, DF_index3, how='outer', on=['SNP'])\n",
    "    \n",
    "# merge DF with fourth index timepoint \n",
    "if DF_index.empty or DF_index4.empty: \n",
    "    pass\n",
    "else: \n",
    "    DF_index = pd.merge(DF_index, DF_index4, how='outer', on=['SNP'])\n",
    "    \n",
    "# merge contact1 with contact2 timepoint \n",
    "if DF_contact1.empty or DF_contact2.empty: \n",
    "    pass\n",
    "else: \n",
    "    DF_contact = pd.merge(DF_contact1, DF_contact2, how='outer', on=['SNP'])\n",
    "    \n",
    "# merge DF with third contact timepoint \n",
    "if DF_contact.empty or DF_contact3.empty: \n",
    "    pass\n",
    "else: \n",
    "    DF_contact = pd.merge(DF_contact, DF_contact3, how='outer', on=['SNP'])\n",
    "    \n",
    "# merge DF with fourth contact timepoint \n",
    "if DF_contact.empty or DF_contact4.empty: \n",
    "    pass\n",
    "else: \n",
    "    DF_contact = pd.merge(DF_contact, DF_contact4, how='outer', on=['SNP'])\n",
    "\n",
    "DF_index\n",
    "DF_contact\n",
    "\n",
    "DF_index = DF_index.replace(np.nan, 0)\n",
    "DF_contact = DF_contact.replace(np.nan, 0)\n",
    "    \n",
    "DF_index.to_csv('../data_derived/iSNVs-over-time/' + index + '.csv', sep='\\t')\n",
    "DF_contact.to_csv('../data_derived/iSNVs-over-time/' + contact + '.csv', sep='\\t')\n",
    "\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge the pairs together \n",
    "\n",
    "Run this after all of the individual ferret CSVs have been generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anhui_27 = pd.read_csv('../data_derived/iSNVs-over-time/Anhui_ferret27.csv', sep='\\t')\n",
    "Anhui_27 = Anhui_27.rename(columns={'day1':'day1_index', 'day3':'day3_index', 'day5':'day5_index', 'day7':'day7_index'})\n",
    "\n",
    "Anhui_28 = pd.read_csv('../data_derived/iSNVs-over-time/Anhui_ferret28.csv', sep='\\t')\n",
    "Anhui_28 = Anhui_28.rename(columns={'day3':'day3_contact', 'day5':'day5_contact'})\n",
    "\n",
    "Anhui_pair1 = pd.merge(Anhui_27, Anhui_28, how='outer', on=['SNP'])\n",
    "Anhui_pair1 = Anhui_pair1.replace(np.nan, 0)\n",
    "\n",
    "Anhui_pair1 = Anhui_pair1.drop('Unnamed: 0_x', 1)\n",
    "Anhui_pair1 = Anhui_pair1.drop('Unnamed: 0_y', 1)\n",
    "\n",
    "Anhui_pair1.to_csv('../data_derived/iSNVs-over-time/pair_1_Anhui.csv', sep='\\t')\n",
    "\n",
    "# Anhui_pair1\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA04_35 = pd.read_csv('../data_derived/iSNVs-over-time/CA04_ferret35.csv', sep='\\t')\n",
    "CA04_35 = CA04_35.rename(columns={'day1':'day1_index', 'day3':'day3_index', 'day5':'day5_index'})\n",
    "\n",
    "CA04_36 = pd.read_csv('../data_derived/iSNVs-over-time/CA04_ferret36.csv', sep='\\t')\n",
    "CA04_36 = CA04_36.rename(columns={'day3':'day3_contact', 'day5':'day5_contact'})\n",
    "\n",
    "CA04_pair1 = pd.merge(CA04_35, CA04_36, how='outer', on=['SNP'])\n",
    "CA04_pair1 = CA04_pair1.replace(np.nan, 0)\n",
    "\n",
    "CA04_pair1 = CA04_pair1.drop('Unnamed: 0_x', 1)\n",
    "CA04_pair1 = CA04_pair1.drop('Unnamed: 0_y', 1)\n",
    "\n",
    "CA04_pair1.to_csv('../data_derived/iSNVs-over-time/pair_1_CA04.csv', sep='\\t')\n",
    "\n",
    "# CA04_pair1\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA04_1 = pd.read_csv('../data_derived/iSNVs-over-time/CA04_ferret1.csv', sep='\\t')\n",
    "CA04_1 = CA04_1.rename(columns={'day1':'day1_index', 'day3':'day3_index', 'day5':'day5_index'})\n",
    "\n",
    "CA04_2 = pd.read_csv('../data_derived/iSNVs-over-time/CA04_ferret2.csv', sep='\\t')\n",
    "CA04_2 = CA04_2.rename(columns={'day1':'day1_contact', 'day3':'day3_contact', 'day5':'day5_contact'})\n",
    "\n",
    "CA04_pair2 = pd.merge(CA04_1, CA04_2, how='outer', on=['SNP'])\n",
    "CA04_pair2 = CA04_pair2.replace(np.nan, 0)\n",
    "\n",
    "CA04_pair2 = CA04_pair2.drop('Unnamed: 0_x', 1)\n",
    "CA04_pair2 = CA04_pair2.drop('Unnamed: 0_y', 1)\n",
    "\n",
    "CA04_pair2.to_csv('../data_derived/iSNVs-over-time/pair_2_CA04.csv', sep='\\t')\n",
    "\n",
    "# CA04_pair2\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA04_3 = pd.read_csv('../data_derived/iSNVs-over-time/CA04_ferret3.csv', sep='\\t')\n",
    "CA04_3 = CA04_3.rename(columns={'day1':'day1_index', 'day3':'day3_index', 'day5':'day5_index'})\n",
    "\n",
    "CA04_4 = pd.read_csv('../data_derived/iSNVs-over-time/CA04_ferret4.csv', sep='\\t')\n",
    "CA04_4 = CA04_4.rename(columns={'day3':'day3_contact', 'day5':'day5_contact'})\n",
    "\n",
    "CA04_pair3 = pd.merge(CA04_3, CA04_4, how='outer', on=['SNP'])\n",
    "CA04_pair3 = CA04_pair3.replace(np.nan, 0)\n",
    "\n",
    "CA04_pair3 = CA04_pair3.drop('Unnamed: 0_x', 1)\n",
    "CA04_pair3 = CA04_pair3.drop('Unnamed: 0_y', 1)\n",
    "\n",
    "CA04_pair3.to_csv('../data_derived/iSNVs-over-time/pair_3_CA04.csv', sep='\\t')\n",
    "\n",
    "# CA04_pair3\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA04_5 = pd.read_csv('../data_derived/iSNVs-over-time/CA04_ferret5.csv', sep='\\t')\n",
    "CA04_5 = CA04_5.rename(columns={'day1':'day1_index', 'day3':'day3_index', 'day5':'day5_index'})\n",
    "\n",
    "CA04_6 = pd.read_csv('../data_derived/iSNVs-over-time/CA04_ferret6.csv', sep='\\t')\n",
    "CA04_6 = CA04_6.rename(columns={'day3':'day3_contact', 'day5':'day5_contact'})\n",
    "\n",
    "CA04_pair4 = pd.merge(CA04_5, CA04_6, how='outer', on=['SNP'])\n",
    "CA04_pair4 = CA04_pair4.replace(np.nan, 0)\n",
    "\n",
    "CA04_pair4 = CA04_pair4.drop('Unnamed: 0_x', 1)\n",
    "CA04_pair4 = CA04_pair4.drop('Unnamed: 0_y', 1)\n",
    "\n",
    "CA04_pair4.to_csv('../data_derived/iSNVs-over-time/pair_4_CA04.csv', sep='\\t')\n",
    "\n",
    "# CA04_pair4\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD3_3 = pd.read_csv('../data_derived/iSNVs-over-time/GD3_ferret3.csv', sep='\\t')\n",
    "GD3_3 = GD3_3.rename(columns={'day1':'day1_index', 'day3':'day3_index', 'day5':'day5_index', 'day7':'day7_index'})\n",
    "\n",
    "GD3_4 = pd.read_csv('../data_derived/iSNVs-over-time/GD3_ferret4.csv', sep='\\t')\n",
    "GD3_4 = GD3_4.rename(columns={'day5':'day5_contact', 'day7':'day7_contact', 'day9':'day9_contact', 'day11':'day11_contact'})\n",
    "\n",
    "GD3_pair1 = pd.merge(GD3_3, GD3_4, how='outer', on=['SNP'])\n",
    "GD3_pair1 = GD3_pair1.replace(np.nan, 0)\n",
    "\n",
    "GD3_pair1 = GD3_pair1.drop('Unnamed: 0_x', 1)\n",
    "GD3_pair1 = GD3_pair1.drop('Unnamed: 0_y', 1)\n",
    "\n",
    "GD3_pair1.to_csv('../data_derived/iSNVs-over-time/pair_1_GD3.csv', sep='\\t')\n",
    "\n",
    "# GD3_pair1\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rGD3_9 = pd.read_csv('../data_derived/iSNVs-over-time/rGD3_ferret9.csv', sep='\\t')\n",
    "rGD3_9 = rGD3_9.rename(columns={'day1':'day1_index', 'day3':'day3_index', 'day5':'day5_index', 'day7':'day7_index'})\n",
    "\n",
    "rGD3_10 = pd.read_csv('../data_derived/iSNVs-over-time/rGD3_ferret10.csv', sep='\\t')\n",
    "rGD3_10 = rGD3_10.rename(columns={'day3':'day3_contact', 'day5':'day5_contact', 'day7':'day7_contact', 'day9':'day9_contact'})\n",
    "\n",
    "rGD3_pair1 = pd.merge(rGD3_9, rGD3_10, how='outer', on=['SNP'])\n",
    "rGD3_pair1 = rGD3_pair1.replace(np.nan, 0)\n",
    "\n",
    "rGD3_pair1 = rGD3_pair1.drop('Unnamed: 0_x', 1)\n",
    "rGD3_pair1 = rGD3_pair1.drop('Unnamed: 0_y', 1)\n",
    "\n",
    "rGD3_pair1.to_csv('../data_derived/iSNVs-over-time/pair_1_rGD3.csv', sep='\\t')\n",
    "\n",
    "# rGD3_pair1\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rGD3_11 = pd.read_csv('../data_derived/iSNVs-over-time/rGD3_ferret11.csv', sep='\\t')\n",
    "rGD3_11 = rGD3_11.rename(columns={'day1':'day1_index', 'day3':'day3_index'})\n",
    "\n",
    "rGD3_12 = pd.read_csv('../data_derived/iSNVs-over-time/rGD3_ferret12.csv', sep='\\t')\n",
    "rGD3_12 = rGD3_12.rename(columns={'day3':'day3_contact', 'day5':'day5_contact'})\n",
    "\n",
    "rGD3_pair2 = pd.merge(rGD3_11, rGD3_12, how='outer', on=['SNP'])\n",
    "rGD3_pair2 = rGD3_pair2.replace(np.nan, 0)\n",
    "\n",
    "rGD3_pair2 = rGD3_pair2.drop('Unnamed: 0_x', 1)\n",
    "rGD3_pair2 = rGD3_pair2.drop('Unnamed: 0_y', 1)\n",
    "\n",
    "rGD3_pair2.to_csv('../data_derived/iSNVs-over-time/pair_2_rGD3.csv', sep='\\t')\n",
    "\n",
    "# rGD3_pair2\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rGD3_15 = pd.read_csv('../data_derived/iSNVs-over-time/rGD3_ferret15.csv', sep='\\t')\n",
    "rGD3_15 = rGD3_15.rename(columns={'day1':'day1_index', 'day3':'day3_index', 'day5':'day5_index', 'day7':'day7_index'})\n",
    "\n",
    "rGD3_16 = pd.read_csv('../data_derived/iSNVs-over-time/rGD3_ferret16.csv', sep='\\t')\n",
    "rGD3_16 = rGD3_16.rename(columns={'day3':'day3_contact', 'day5':'day5_contact', 'day7':'day7_contact'})\n",
    "\n",
    "rGD3_pair3 = pd.merge(rGD3_15, rGD3_16, how='outer', on=['SNP'])\n",
    "rGD3_pair3 = rGD3_pair3.replace(np.nan, 0)\n",
    "\n",
    "rGD3_pair3 = rGD3_pair3.drop('Unnamed: 0_x', 1)\n",
    "rGD3_pair3 = rGD3_pair3.drop('Unnamed: 0_y', 1)\n",
    "\n",
    "rGD3_pair3.to_csv('../data_derived/iSNVs-over-time/pair_3_rGD3.csv', sep='\\t')\n",
    "\n",
    "# rGD3_pair3\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rGD3_17 = pd.read_csv('../data_derived/iSNVs-over-time/rGD3_ferret17.csv', sep='\\t')\n",
    "rGD3_17 = rGD3_17.rename(columns={'day1':'day1_index', 'day3':'day3_index', 'day5':'day5_index', 'day7':'day7_index'})\n",
    "\n",
    "rGD3_18 = pd.read_csv('../data_derived/iSNVs-over-time/rGD3_ferret18.csv', sep='\\t')\n",
    "rGD3_18 = rGD3_18.rename(columns={'day11':'day11_contact', 'day13':'day13_contact', 'day15':'day15_contact'})\n",
    "\n",
    "rGD3_pair4 = pd.merge(rGD3_17, rGD3_18, how='outer', on=['SNP'])\n",
    "rGD3_pair4 = rGD3_pair4.replace(np.nan, 0)\n",
    "\n",
    "rGD3_pair4 = rGD3_pair4.drop('Unnamed: 0_x', 1)\n",
    "rGD3_pair4 = rGD3_pair4.drop('Unnamed: 0_y', 1)\n",
    "\n",
    "rGD3_pair4.to_csv('../data_derived/iSNVs-over-time/pair_4_rGD3.csv', sep='\\t')\n",
    "\n",
    "# rGD3_pair4\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rGD3_21 = pd.read_csv('../data_derived/iSNVs-over-time/rGD3_ferret21.csv', sep='\\t')\n",
    "rGD3_21 = rGD3_21.rename(columns={'day1':'day1_index', 'day3':'day3_index', 'day5':'day5_index'})\n",
    "\n",
    "rGD3_22 = pd.read_csv('../data_derived/iSNVs-over-time/rGD3_ferret22.csv', sep='\\t')\n",
    "rGD3_22 = rGD3_22.rename(columns={'day3':'day3_contact', 'day5':'day5_contact', 'day7':'day7_contact', 'day9':'day9_contact'})\n",
    "\n",
    "rGD3_pair5 = pd.merge(rGD3_21, rGD3_22, how='outer', on=['SNP'])\n",
    "rGD3_pair5 = rGD3_pair5.replace(np.nan, 0)\n",
    "\n",
    "rGD3_pair5 = rGD3_pair5.drop('Unnamed: 0_x', 1)\n",
    "rGD3_pair5 = rGD3_pair5.drop('Unnamed: 0_y', 1)\n",
    "\n",
    "rGD3_pair5.to_csv('../data_derived/iSNVs-over-time/pair_5_rGD3.csv', sep='\\t')\n",
    "\n",
    "# rGD3_pair5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
